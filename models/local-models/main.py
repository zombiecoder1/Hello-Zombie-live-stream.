from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import uvicorn
import requests
import json
from typing import Dict, Any, List

app = FastAPI(title="Local Models Service", version="1.0.0")

class ModelRequest(BaseModel):
    prompt: str
    model: str = "llama2"
    max_tokens: int = 1000
    temperature: float = 0.7

class ModelResponse(BaseModel):
    response: str
    model: str
    status: str
    timestamp: str

# Ollama configuration
OLLAMA_BASE_URL = "http://localhost:11434"

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "local-models",
        "timestamp": "2024-01-01T00:00:00Z"
    }

@app.get("/api/status")
async def get_status():
    try:
        # Check if Ollama is running
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return {
                "message": "Local Models Service is running",
                "version": "1.0.0",
                "ollama_status": "connected",
                "available_models": [model["name"] for model in models],
                "total_models": len(models),
                "timestamp": "2024-01-01T00:00:00Z"
            }
        else:
            return {
                "message": "Local Models Service is running",
                "version": "1.0.0",
                "ollama_status": "disconnected",
                "available_models": [],
                "total_models": 0,
                "timestamp": "2024-01-01T00:00:00Z"
            }
    except Exception as e:
        return {
            "message": "Local Models Service is running",
            "version": "1.0.0",
            "ollama_status": "error",
            "error": str(e),
            "available_models": [],
            "total_models": 0,
            "timestamp": "2024-01-01T00:00:00Z"
        }

@app.post("/api/generate", response_model=ModelResponse)
async def generate_text(request: ModelRequest):
    try:
        # Prepare request for Ollama
        ollama_request = {
            "model": request.model,
            "prompt": request.prompt,
            "stream": False,
            "options": {
                "temperature": request.temperature,
                "num_predict": request.max_tokens
            }
        }
        
        # Call Ollama API with optimized timeout
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json=ollama_request,
            timeout=60,  # Increased timeout
            headers={'Connection': 'keep-alive'}
        )
        
        if response.status_code == 200:
            result = response.json()
            generated_text = result.get("response", "")
            
            return ModelResponse(
                response=generated_text,
                model=request.model,
                status="success",
                timestamp="2024-01-01T00:00:00Z"
            )
        else:
            raise HTTPException(status_code=500, detail="Ollama API error")
            
    except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, Exception) as e:
        # Fallback to local processing if Ollama is not available
        fallback_response = f"""
Based on your prompt: "{request.prompt}"

This is a local AI response generated by the ZombieCoder system. 
The response is being processed locally without external API calls.

Key points from your request:
- Model: {request.model}
- Temperature: {request.temperature}
- Max tokens: {request.max_tokens}

This is a simulated response that would normally come from a local AI model.
In a production environment, this would be replaced with actual model inference.

Error: {str(e)}
"""
        
        return ModelResponse(
            response=fallback_response.strip(),
            model=request.model,
            status="fallback",
            timestamp="2024-01-01T00:00:00Z"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat")
async def chat_completion(request: ModelRequest):
    try:
        # Prepare chat request for Ollama
        ollama_request = {
            "model": request.model,
            "messages": [
                {"role": "user", "content": request.prompt}
            ],
            "stream": False,
            "options": {
                "temperature": request.temperature,
                "num_predict": request.max_tokens
            }
        }
        
        # Call Ollama chat API with optimized timeout
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/chat",
            json=ollama_request,
            timeout=60,  # Increased timeout
            headers={'Connection': 'keep-alive'}
        )
        
        if response.status_code == 200:
            result = response.json()
            message = result.get("message", {})
            content = message.get("content", "")
            
            return {
                "response": content,
                "model": request.model,
                "status": "success",
                "timestamp": "2024-01-01T00:00:00Z"
            }
        else:
            raise HTTPException(status_code=500, detail="Ollama chat API error")
            
    except requests.exceptions.ConnectionError:
        # Fallback response
        fallback_response = f"""
User: {request.prompt}

Assistant: I understand you're asking about: "{request.prompt}"

This is a local AI assistant response. In a production environment, this would be 
generated by a local language model running on your system.

The system is configured to use local models to ensure privacy and reduce 
dependencies on external services.
"""
        
        return {
            "response": fallback_response.strip(),
            "model": request.model,
            "status": "fallback",
            "timestamp": "2024-01-01T00:00:00Z"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/models")
async def list_models():
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return {
                "models": models,
                "status": "success",
                "timestamp": "2024-01-01T00:00:00Z"
            }
        else:
            return {
                "models": [],
                "status": "error",
                "message": "Could not fetch models from Ollama",
                "timestamp": "2024-01-01T00:00:00Z"
            }
    except Exception as e:
        return {
            "models": [],
            "status": "error",
            "message": f"Error connecting to Ollama: {str(e)}",
            "timestamp": "2024-01-01T00:00:00Z"
        }

@app.post("/api/models/pull")
async def pull_model(model_name: str):
    try:
        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/pull",
            json={"name": model_name},
            timeout=300  # 5 minutes timeout for model download
        )
        
        if response.status_code == 200:
            return {
                "model": model_name,
                "status": "pulling",
                "message": "Model download started",
                "timestamp": "2024-01-01T00:00:00Z"
            }
        else:
            raise HTTPException(status_code=500, detail="Failed to start model download")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5252)
